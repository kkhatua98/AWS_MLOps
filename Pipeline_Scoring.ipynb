{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c33749c4",
   "metadata": {},
   "source": [
    "# Prework\n",
    "#### Importing Libraries and Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d178531",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "\n",
    "## Loading the configurations from config.json file.\n",
    "import json\n",
    "with open(\"config.json\") as file:\n",
    "    build_parameters = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90099b5",
   "metadata": {},
   "source": [
    "#### Setting Default Bucket and getting region and role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "685ad581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::852619674999:role/service-role/AmazonSageMaker-ExecutionRole-20220427T124311\n",
      "churn-output-bucket-us-east-1\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "pipeline_output_bucket = build_parameters[\"output_bucket\"] \n",
    "sagemaker_session = sagemaker.session.Session(default_bucket = pipeline_output_bucket)\n",
    "# role = sagemaker.get_execution_role()\n",
    "role = \"arn:aws:iam::852619674999:role/service-role/AmazonSageMaker-ExecutionRole-20220427T124311\"\n",
    "\n",
    "print(role)\n",
    "print(sagemaker_session.default_bucket())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef6fc64",
   "metadata": {},
   "source": [
    "#### Input Data Location Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69769b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Default location for the datasets\n",
    "input_bucket = build_parameters[\"input_bucket\"]\n",
    "batch_data_uri = build_parameters[\"scoring_data_s3_location\"]\n",
    "feature_selection_file_uri = build_parameters[\"feature_selection\"]\n",
    "\n",
    "# Parametrizing Data paths\n",
    "from sagemaker.workflow.parameters import ParameterInteger, ParameterString\n",
    "batch_data = ParameterString(name=\"BatchData\", default_value=batch_data_uri)\n",
    "feature_selection_file = ParameterString(name = \"FeatureSelectionFile\", default_value = feature_selection_file_uri)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c6aeff",
   "metadata": {},
   "source": [
    "#### Handling Output Locations\n",
    "See this link to learn more about pipeline execution variables: https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html#sagemaker.workflow.execution_variables.ExecutionVariables\n",
    "pipeline_start_time is a execution vairable, so to create processig_output_path and inference_output_path we had to use sagemaker.workflow.functions.Join and we could not use Python f-strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f07f866",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipeline_output_bucket = build_parameters[\"output_bucket\"]\n",
    "\n",
    "pipeline_start_time = sagemaker.workflow.execution_variables.ExecutionVariables.START_DATETIME\n",
    "\n",
    "from sagemaker.workflow import functions\n",
    "processing_output_path = functions.Join(on='/', values=[\"s3:/\", pipeline_output_bucket, \"Scoring_Pipeline_Output\", pipeline_start_time, \"ProcessingOutput\"])\n",
    "inference_output_path = functions.Join(on='/', values=[\"s3:/\", pipeline_output_bucket, \"Scoring_Pipeline_Output\", pipeline_start_time, \"InferenceOutput\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f87c7a5",
   "metadata": {},
   "source": [
    "# Building the Pipeline Steps\n",
    "### Step 1: Building the Preprocessing Component\n",
    "#### Building the Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e69999e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "framework_version = build_parameters[\"sklearn_processor_framework_version\"]\n",
    "\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version=framework_version,\n",
    "    instance_type=build_parameters[\"scoring_preprocessing_instance_type\"],\n",
    "    instance_count=build_parameters[\"scoring_preprocessing_instance_count\"],\n",
    "    base_job_name=\"Churn-Inference-Preprocessing\",\n",
    "    role=role\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72311d8d",
   "metadata": {},
   "source": [
    "#### Building the Processing Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbe7aa2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep, TuningStep\n",
    "    \n",
    "\n",
    "step_process = ProcessingStep(\n",
    "    name=\"Preprocessing\",\n",
    "    processor=sklearn_processor,\n",
    "    inputs=[\n",
    "      ProcessingInput(source=batch_data, destination=\"/opt/ml/processing/input\"),  \n",
    "      ProcessingInput(source=feature_selection_file, destination=\"/opt/ml/processing/input/feature_selection\")\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train\", source=\"/opt/ml/processing/train\", \n",
    "                         destination = processing_output_path\n",
    "                        ),\n",
    "        ProcessingOutput(output_name = \"logs\", source=\"/opt/ml/processing/logss\", \n",
    "                         destination = processing_output_path\n",
    "                        )\n",
    "    ],\n",
    "    code = os.path.join(\"Pipeline_Component_Codes\", \"Scoring\", \"1_Preprocessing\", \"Scoring_Preprocessing.py\"),\n",
    "    job_arguments = [\"--batch_data_location\", \"/opt/ml/processing/input\", \"--target_column\", \"Churn\",\n",
    "                     \"--feature_selection_file_location\", \"/opt/ml/processing/input/feature_selection\",\n",
    "                     \"--preprocessed_batch_data_location\", \"/opt/ml/processing/train\", \"--log_location\", \"/opt/ml/processing/logss\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a07d88c",
   "metadata": {},
   "source": [
    "### Step 2: Get Model Step\n",
    "#### Building the Lambda Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3f47d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.lambda_helper import Lambda\n",
    "\n",
    "func = Lambda(\n",
    "    function_name = \"get_model_from_registry\",\n",
    "    execution_role_arn=\"arn:aws:iam::852619674999:role/role_given_to_lambda\",\n",
    "    script = os.path.join(\"Pipeline_Component_Codes\", \"Scoring\", \"2_Get_Model\", \"main.py\"),\n",
    "    handler=\"main.main\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2da26b",
   "metadata": {},
   "source": [
    "#### Building the Lambdastep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "380697bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.lambda_step import LambdaOutput, LambdaStep, LambdaOutputTypeEnum\n",
    "\n",
    "output_param_1 = LambdaOutput(output_name=\"image_uri\", output_type=LambdaOutputTypeEnum.String)\n",
    "output_param_2 = LambdaOutput(output_name=\"model_data_uri\", output_type=LambdaOutputTypeEnum.String)\n",
    "output_param_3 = LambdaOutput(output_name=\"instance_type\", output_type=LambdaOutputTypeEnum.String)\n",
    "\n",
    "get_model_step = LambdaStep(\n",
    "    name=f\"get_model\",\n",
    "    lambda_func=func,\n",
    "    inputs={\n",
    "        \"model_package_group_name\": build_parameters[\"model_package_group_name\"]\n",
    "    },\n",
    "    outputs=[output_param_1, output_param_2, output_param_3]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dde972",
   "metadata": {},
   "source": [
    "### Step 3: Making Inference Step\n",
    "As mentioned earlier this step is created using a ProcessingStep\n",
    "#### Building the processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32b49db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import Processor\n",
    "processor = Processor(\n",
    "    image_uri = get_model_step.properties.Outputs[\"image_uri\"],\n",
    "    instance_type = get_model_step.properties.Outputs[\"instance_type\"],\n",
    "    instance_count = 1,\n",
    "    base_job_name = f\"inference-preprocessing\",\n",
    "    role=role\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c88a6dc",
   "metadata": {},
   "source": [
    "#### Building the Inference Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cad92f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_inference = ProcessingStep(\n",
    "    name=\"Inference\",\n",
    "    # processor=processor,\n",
    "    processor=sklearn_processor,\n",
    "    inputs=[\n",
    "      ProcessingInput(source=step_process.properties.ProcessingOutputConfig.Outputs[\"train\"].S3Output.S3Uri, destination=\"/opt/ml/processing/input/data\"),  \n",
    "      ProcessingInput(source=get_model_step.properties.Outputs[\"model_data_uri\"], destination=\"/opt/ml/processing/input/model_folder\"),  \n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train\", source=\"/opt/ml/processing/train\", destination = inference_output_path)\n",
    "    ],\n",
    "    code = os.path.join(\"Pipeline_Component_Codes\", \"Scoring\", \"3_Scoring\", \"scoring.py\"),\n",
    "    job_arguments = [\"--batch_data_location\", \"/opt/ml/processing/input/data\", \n",
    "                     \"--model_location\", \"/opt/ml/processing/input/model_folder\",\n",
    "                     \"--predicted_data_location\", \"/opt/ml/processing/train\", \n",
    "                     \"--log_location\", \"/opt/ml/processing/logss\"\n",
    "                    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3c22614",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from sagemaker.transformer import Transformer\n",
    "\n",
    "\n",
    "# transformer = Transformer(\n",
    "#     model_name=step_create_model.properties.ModelName,\n",
    "#     instance_type=build_parameters[\"scoring_instance_type\"],\n",
    "#     instance_count=1,\n",
    "#     output_path=inference_output_path,\n",
    "#     base_transform_job_name = \"Churn-Transformation\",\n",
    "#     # max_concurrent_transforms = 1,\n",
    "#     # strategy = \"SingleRecord\"\n",
    "# )\n",
    "\n",
    "# from sagemaker.inputs import TransformInput\n",
    "# from sagemaker.workflow.steps import TransformStep\n",
    "\n",
    "\n",
    "# step_transform = TransformStep(\n",
    "#     name=\"Inference\",\n",
    "#     transformer=transformer,\n",
    "#     inputs=TransformInput(data=step_process.properties.ProcessingOutputConfig.Outputs[\"train\"].S3Output.S3Uri,\n",
    "#                           # data_type = \"text/csv\"\n",
    "#                          ),\n",
    "#     depends_on  = [step_process]\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25cc908",
   "metadata": {},
   "source": [
    "# Building the Pipeline\n",
    "#### Arranging the steps inside pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eface464",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "pipeline_name = f\"Churn-Scoring\"\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        batch_data,\n",
    "        feature_selection_file\n",
    "    ],\n",
    "    steps=[step_process, \n",
    "           get_model_step, \n",
    "           step_inference\n",
    "          ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd43468",
   "metadata": {},
   "source": [
    "#### Uploading the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8317187b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:us-east-1:852619674999:pipeline/churn-scoring',\n",
       " 'ResponseMetadata': {'RequestId': '87462b97-1b04-4a14-b0ac-fe33ad8c0125',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '87462b97-1b04-4a14-b0ac-fe33ad8c0125',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '81',\n",
       "   'date': 'Fri, 19 May 2023 10:27:18 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.upsert(role_arn=role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac59c8c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
