{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fafd2cbd",
   "metadata": {},
   "source": [
    "# Importing Libraries and Configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f71670",
   "metadata": {},
   "source": [
    "https://github.com/aws/amazon-sagemaker-examples/issues/1207\n",
    "\n",
    "\n",
    "https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost-tuning.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "841b720e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "\n",
    "# Taking pipeline building configurations from config.json.\n",
    "# These are only for building and will not be available at \n",
    "# the runtime of the pipeline.\n",
    "with open(\"config.json\") as file:\n",
    "    build_parameters = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5b2e9e",
   "metadata": {},
   "source": [
    "# Setting Default Bucket and getting region and role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b9de92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "churn-output-bucket-us-east-1\n",
      "churn-output-bucket-us-east-1\n",
      "arn:aws:iam::852619674999:role/service-role/AmazonSageMaker-ExecutionRole-20220427T124311\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## Setting default bucket\n",
    "# Method 1\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "pipeline_session = PipelineSession(default_bucket = build_parameters[\"output_bucket\"])\n",
    "sagemaker_session = sagemaker.Session(default_bucket = build_parameters[\"output_bucket\"])\n",
    "\n",
    "# Method 2\n",
    "pipeline_session.default_bucket = build_parameters[\"output_bucket\"]\n",
    "sagemaker_session.default_bucket = build_parameters[\"output_bucket\"]\n",
    "\n",
    "# Method 3\n",
    "# sagemaker_session = sagemaker.Session()\n",
    "# bucket = \"sagemaker-output-bucket-us-east1\"   \n",
    "\n",
    "print(sagemaker_session.default_bucket)\n",
    "print(pipeline_session.default_bucket)\n",
    "\n",
    "\n",
    "## Getting region and role\n",
    "region = boto3.Session().region_name\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "print(role)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5343671b",
   "metadata": {},
   "source": [
    "# Input Data Location Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f2cf30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Default location for the datasets\n",
    "train_data_uri = build_parameters[\"train_data\"]\n",
    "test_data_uri = build_parameters[\"test_data\"]\n",
    "evaluation_data_uri = build_parameters[\"evaluation_data\"]\n",
    "feature_selection_file_uri = build_parameters[\"feature_selection\"]\n",
    "\n",
    "\n",
    "# Parametrizing Data paths\n",
    "from sagemaker.workflow.parameters import ParameterInteger, ParameterString\n",
    "train_data = ParameterString(name=\"TrainData\", default_value = train_data_uri)\n",
    "test_data = ParameterString(name=\"TestData\", default_value = test_data_uri)\n",
    "evaluation_data = ParameterString(name=\"EvaluationData\", default_value = evaluation_data_uri)\n",
    "feature_selection_file = ParameterString(name = \"FeatureSelectionFile\", default_value = feature_selection_file_uri)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73b964a",
   "metadata": {},
   "source": [
    "# Preprocessing Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6335b53f",
   "metadata": {},
   "source": [
    "#### 2.1 Loading preprocessing config.json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f51c30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_preprocessing_path = os.path.join(\"Pipeline_Component_Codes\",\"Training\",\"1_Preprocessing\")\n",
    "with open(os.path.join(local_preprocessing_path, \"config.json\")) as file:\n",
    "    processing_build_parameters = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ba34bf",
   "metadata": {},
   "source": [
    "#### 2.2 Making parameter for processing machine type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87399e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_instance_type = ParameterString(\n",
    "    name=\"ProcessingInstanceType\",\n",
    "    default_value=processing_build_parameters[\"machine_type\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ee1acc",
   "metadata": {},
   "source": [
    "#### 2.3 Building the processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53ac9b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if processing_build_parameters[\"processing_type\"] == \"sklearn\":\n",
    "    from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "    processor = SKLearnProcessor(\n",
    "        framework_version = processing_build_parameters[\"framework_version\"],\n",
    "        instance_type = processing_build_parameters[\"machine_type\"],\n",
    "        instance_count = processing_build_parameters[\"machine_count\"],\n",
    "        base_job_name = f\"{build_parameters['usecase']}-preprocessing\",\n",
    "        role=role\n",
    "    )\n",
    "elif processing_build_parameters[\"processing_type\"] == \"custom\":\n",
    "    from sagemaker.processor import Processor\n",
    "    processor = Processor(\n",
    "        image_uri = processing_build_parameters[\"image_uri\"],\n",
    "        instance_type = processing_build_parameters[\"machine_type\"],\n",
    "        instance_count = processing_build_parameters[\"machine_count\"],\n",
    "        base_job_name = f\"{build_parameters['usecase']}-preprocessing\",\n",
    "        role=role\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb464ac",
   "metadata": {},
   "source": [
    "#### 2.4 Building preprocessing step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39f92871",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "    \n",
    "\n",
    "step_process = ProcessingStep(\n",
    "    name = \"preprocessing_full_data\",\n",
    "    description = \"Data preprocessing and splitting into train and test set\",\n",
    "    processor=processor,\n",
    "    inputs=[\n",
    "        ProcessingInput(source = train_data, destination=\"/opt/ml/processing/input/data\"),  \n",
    "        ProcessingInput(source=feature_selection_file, destination=\"/opt/ml/processing/input/feature_selection\")\n",
    "    ],\n",
    "    outputs=[\n",
    "        # Train\n",
    "        ProcessingOutput(output_name = \"train\", source=\"/opt/ml/processing/train\", \n",
    "                         destination = sagemaker_session.default_bucket\n",
    "#                          destination = sagemaker.workflow.functions.Join(on='/', values = [processing_output_path, \"data\"])\n",
    "                        ),\n",
    "        # Test\n",
    "        ProcessingOutput(output_name = \"test\", source=\"/opt/ml/processing/test\", \n",
    "                         destination = sagemaker_session.default_bucket\n",
    "#                          destination = sagemaker.workflow.functions.Join(on='/', values = [processing_output_path, \"data\"])\n",
    "                        ),\n",
    "        # Logs\n",
    "        ProcessingOutput(output_name = \"logs\", source=\"/opt/ml/processing/logss\", \n",
    "                         destination = sagemaker_session.default_bucket\n",
    "#                          destination = sagemaker.workflow.functions.Join(on='/', values = [processing_output_path, \"logs\"])\n",
    "                        ),\n",
    "    ],\n",
    "#     code=\"SageMaker_Pipeline_Component_Codes/Training/Training_Preprocessing.py\",\n",
    "    code=os.path.join(local_preprocessing_path, processing_build_parameters[\"entry_point\"]),\n",
    "    job_arguments = [\"--train_data_location\", \"/opt/ml/processing/input/data\", \n",
    "                     \"--feature_selection_file_location\", \"/opt/ml/processing/input/feature_selection/Feature_Selection.csv\", \n",
    "                     \"--target_column\", \"Churn\",\n",
    "                     \"--preprocessed_train_data_location\", \"/opt/ml/processing/train\", \n",
    "                     \"--preprocessed_test_data_location\", \"/opt/ml/processing/test\", \n",
    "                     \"--log_location\", \"/opt/ml/processing/logss\"\n",
    "                    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2236d1",
   "metadata": {},
   "source": [
    "# 3. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa6d0ba",
   "metadata": {},
   "source": [
    "#### 3.1 Getting models from Local Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c84d2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Decision_Tree', 'Logistic_Regression']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "local_models_path = os.path.join(\"Pipeline_Component_Codes\", \"Training\", \"2_Models_HPTune\")\n",
    "models = []\n",
    "for directory in os.listdir(local_models_path):\n",
    "    if '.' not in directory: # Avoiding .ipynb_checkpoints\n",
    "        models.append(directory)\n",
    "\n",
    "models = models[:-1]\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818e224d",
   "metadata": {},
   "source": [
    "#### 3.2 Reading config.json Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b263628a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_details = {}\n",
    "for model in models:\n",
    "    with open(os.path.join(local_models_path, model, \"config.json\")) as file:\n",
    "        model_build_parameters = json.load(file)\n",
    "    model_details[model] = model_build_parameters\n",
    "\n",
    "# print(model_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06457eef",
   "metadata": {},
   "source": [
    "#### 3.2 Input Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a2eaa2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine types\n",
    "training_instances = []\n",
    "for model in models:\n",
    "    training_instance_type = ParameterString(\n",
    "        name=f\"{model}_InstanceType\",\n",
    "        default_value=\"ml.m5.xlarge\"\n",
    "    )\n",
    "    training_instances.append(training_instance_type)\n",
    "\n",
    "# Objective metric\n",
    "# objective_metric_name = ParameterString(name = \"ObjectiveMetric\", default_value = build_parameters[\"objective_metric\"])\n",
    "# metric_definitions = [{\"Name\": objective_metric_name, \"Regex\": \"accuracy:([0-9\\\\.]+)\"}]\n",
    "\n",
    "objective_metric_name = \"validation:accuracy\"\n",
    "metric_definitions = [{'Name': \"validation:accuracy\", 'Regex': \"accuracy:([0-9\\\\.]+)\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab5c2df",
   "metadata": {},
   "source": [
    "#### 3.3 Creating the Estimators on which Tuning Will Happen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e4377f",
   "metadata": {},
   "source": [
    "Sagemaker provides us docker conatiners for all the popular algorithms like they have scikit learn image for all the Scikit learn models, they have XGBoost image and they also have deep learning images as well. Here for the demo purpose we have used only three models, Logistic Regression, Decision Tree and XGBoost. So our need was not to build our own image. If any other Python library is needed we can mention those in **requirements.txt** file. We do not have to do anything more, pipeline will automatically install those in respective containers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21d75f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "instance_type is a PipelineVariable (<class 'sagemaker.workflow.parameters.ParameterString'>). Its interpreted value in execution time should not be of GPU types since GPU training is not supported for Scikit-Learn.\n",
      "The input argument instance_type of function (sagemaker.image_uris.retrieve) is a pipeline variable (<class 'sagemaker.workflow.parameters.ParameterString'>), which is not allowed. The default_value of this Parameter object will be used to override it. Please make sure the default_value is valid.\n",
      "instance_type is a PipelineVariable (<class 'sagemaker.workflow.parameters.ParameterString'>). Its interpreted value in execution time should not be of GPU types since GPU training is not supported for Scikit-Learn.\n",
      "The input argument instance_type of function (sagemaker.image_uris.retrieve) is a pipeline variable (<class 'sagemaker.workflow.parameters.ParameterString'>), which is not allowed. The default_value of this Parameter object will be used to override it. Please make sure the default_value is valid.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.sklearn import SKLearn\n",
    "from sagemaker.xgboost.estimator import XGBoost\n",
    "\n",
    "from sagemaker.tuner import ContinuousParameter, IntegerParameter, CategoricalParameter, HyperparameterTuner\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.workflow.steps import TuningStep\n",
    "\n",
    "from sagemaker.workflow import functions\n",
    "\n",
    "\n",
    "\n",
    "tuning_steps = []\n",
    "index = -1\n",
    "for model in models:\n",
    "    index = index + 1\n",
    "    current_model_details = model_details[model]\n",
    "    \n",
    "    # Creating the Estimators on which Tuning Will Happen\n",
    "    if current_model_details['model_type'] == 'sklearn_model':\n",
    "        estimator = SKLearn(\n",
    "                            source_dir = os.path.join(local_models_path, model),\n",
    "                            entry_point = current_model_details[\"entry_point\"], \n",
    "                            instance_type = training_instances[index], \n",
    "                            instance_count = 1,\n",
    "                            framework_version = current_model_details[\"framework_version\"], \n",
    "                            role = role,\n",
    "                            # output_path = f\"{hptune_training_output_path}/{model_details['model_name']}\",\n",
    "                            # output_path = functions.Join(on = '/', values = [pipeline_session.default_bucket, \"churn_training\"])\n",
    "                            )\n",
    "    elif current_model_details['model_type'] == 'xgboost_model':\n",
    "        estimator = XGBoost(\n",
    "                            source_dir = os.path.join(local_models_path, model),\n",
    "                            entry_point = current_model_details[\"entry_point\"],\n",
    "                            instance_type = training_instances[index],\n",
    "                            instance_count = 1,\n",
    "                            framework_version = current_model_details[\"framework_version\"],\n",
    "                            role=role,\n",
    "                            # output_path = f\"{hptune_training_output_path}/{model_details['model_name']}\",\n",
    "                            # output_path = functions.Join(on = '/', values = [pipeline_session.default_bucket, \"churn_training\"])\n",
    "                            )\n",
    "    \n",
    "    \n",
    "    # Getting the hyperparameters\n",
    "    hyperparameters = current_model_details[\"hyperparameters\"]\n",
    "    \n",
    "    hyperparameter_ranges = {}\n",
    "    for hyperparameter in hyperparameters:\n",
    "        if hyperparameters[hyperparameter][\"type\"] == \"continuous\":\n",
    "            hyperparameter_ranges[hyperparameter] = ContinuousParameter(min_value = hyperparameters[hyperparameter][\"min_value\"],\n",
    "                                                                        max_value = hyperparameters[hyperparameter][\"max_value\"],\n",
    "                                                                        scaling_type = hyperparameters[hyperparameter][\"scaling_type\"])\n",
    "        elif hyperparameters[hyperparameter][\"type\"] == \"categorical\":\n",
    "            hyperparameter_ranges[hyperparameter] = CategoricalParameter(hyperparameters[hyperparameter][\"values\"])\n",
    "        elif hyperparameters[hyperparameter][\"type\"] == \"integer\":\n",
    "            hyperparameter_ranges[hyperparameter] = IntegerParameter(min_value = hyperparameters[hyperparameter][\"min_value\"],\n",
    "                                                                     max_value = hyperparameters[hyperparameter][\"max_value\"])\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Making the hyperparameter tuner\n",
    "    tuner = HyperparameterTuner(\n",
    "        estimator = estimator,\n",
    "        objective_metric_name = objective_metric_name,\n",
    "        hyperparameter_ranges = hyperparameter_ranges,\n",
    "        metric_definitions = metric_definitions,\n",
    "        max_jobs=1,\n",
    "        max_parallel_jobs=1,\n",
    "        strategy = current_model_details[\"tuning_strategy\"],\n",
    "        base_tuning_job_name = current_model_details[\"model_name\"]\n",
    "        )\n",
    "    \n",
    "    \n",
    "    # Building the tuning step\n",
    "    step_tuning = TuningStep(\n",
    "        name = f\"hptuning_{current_model_details['model_name']}\",\n",
    "        tuner = tuner,\n",
    "        inputs={\n",
    "            \"train\": TrainingInput(\n",
    "                s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\"train\"].S3Output.S3Uri,\n",
    "                content_type=\"text/csv\"\n",
    "            ),\n",
    "            \"test\": TrainingInput(\n",
    "                s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\"test\"].S3Output.S3Uri,\n",
    "                content_type=\"text/csv\",\n",
    "            ),\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    tuning_steps.append(step_tuning)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cca891c",
   "metadata": {},
   "source": [
    "# 4. Getting the best model from each hyperparameter tuning job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988b071a",
   "metadata": {},
   "source": [
    "Follow this link to get example of how to write lambda step https://github.com/aws/amazon-sagemaker-examples/blob/main/sagemaker-pipelines/tabular/lambda-step/sagemaker-pipelines-lambda-step.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71415d1",
   "metadata": {},
   "source": [
    "#### Building the Lambda Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d490c553",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.lambda_helper import Lambda\n",
    "\n",
    "\n",
    "func = Lambda(\n",
    "    function_name = \"get_best_model_from_hptune_job\",\n",
    "    execution_role_arn=\"arn:aws:iam::852619674999:role/role_given_to_lambda\",\n",
    "#     execution_role_arn = role,\n",
    "#     execution_role_arn = lambda_role,\n",
    "    script = os.path.join(\"Pipeline_Component_Codes\", \"Training\", \"3_Model_Selection\", \"main.py\"),\n",
    "    handler=\"main.main\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ed1749",
   "metadata": {},
   "source": [
    "#### Building the Lambdastep\n",
    "See this link to get the idea on how tuning step name is being fetched (tuning_steps[i].properties.HyperParameterTuningJobName): https://boto3.amazonaws.com/v1/documentation/api/1.9.46/reference/services/sagemaker.html#SageMaker.Client.describe_hyper_parameter_tuning_job \n",
    "In the same way other properties can also be accessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "071290a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.lambda_step import LambdaOutput, LambdaStep, LambdaOutputTypeEnum\n",
    "\n",
    "best_model_steps = []\n",
    "for i in range(len(models)):\n",
    "    # Building the outputs\n",
    "    output_param_1 = LambdaOutput(output_name=f\"best_{models[i]}_model_location\", output_type=LambdaOutputTypeEnum.String)\n",
    "    output_param_2 = LambdaOutput(output_name=f\"best_{models[i]}_metric_value\", output_type=LambdaOutputTypeEnum.String)\n",
    "    \n",
    "    # Building the Lambdastep\n",
    "    step_deploy_lambda = LambdaStep(\n",
    "        name=f\"get_best_{models[i]}_model\",\n",
    "        lambda_func=func,\n",
    "        inputs={\n",
    "            \"tuning_job_name\": tuning_steps[i].properties.HyperParameterTuningJobName\n",
    "        },\n",
    "        outputs=[output_param_1, output_param_2]\n",
    "    )\n",
    "    \n",
    "    best_model_steps.append(step_deploy_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df09a35e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a20b1019",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-d1198eddaa50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcurrent_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%m-%d-%H-%M-%S\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocaltime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"demo-lambda-model\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcurrent_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mendpoint_config_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"demo-lambda-deploy-endpoint-config-\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcurrent_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mendpoint_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"demo-lambda-deploy-endpoint-\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcurrent_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'time' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "current_time = time.strftime(\"%m-%d-%H-%M-%S\", time.localtime())\n",
    "model_name = \"demo-lambda-model\" + current_time\n",
    "endpoint_config_name = \"demo-lambda-deploy-endpoint-config-\" + current_time\n",
    "endpoint_name = \"demo-lambda-deploy-endpoint-\" + current_time\n",
    "\n",
    "function_name = \"sagemaker-lambda-step-endpoint-deploy-\" + current_time\n",
    "\n",
    "# Lambda helper class can be used to create the Lambda function\n",
    "func = Lambda(\n",
    "    function_name=function_name,\n",
    "    execution_role_arn=lambda_role,\n",
    "    script=\"code/lambda_helper.py\",\n",
    "    handler=\"lambda_helper.lambda_handler\",\n",
    ")\n",
    "\n",
    "output_param_1 = LambdaOutput(output_name=\"statusCode\", output_type=LambdaOutputTypeEnum.String)\n",
    "output_param_2 = LambdaOutput(output_name=\"body\", output_type=LambdaOutputTypeEnum.String)\n",
    "output_param_3 = LambdaOutput(output_name=\"other_key\", output_type=LambdaOutputTypeEnum.String)\n",
    "\n",
    "step_deploy_lambda = LambdaStep(\n",
    "    name=\"LambdaStep\",\n",
    "    lambda_func=func,\n",
    "    inputs={\n",
    "        \"model_name\": step_create_model.properties.ModelName,\n",
    "        \"endpoint_config_name\": endpoint_config_name,\n",
    "        \"endpoint_name\": endpoint_name,\n",
    "    },\n",
    "    outputs=[output_param_1, output_param_2, output_param_3],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d73f2dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "from sagemaker.inputs import CreateModelInput\n",
    "inputs = CreateModelInput(\n",
    "    instance_type=build_parameters[\"evaluation_instance_type\"],\n",
    "    # accelerator_type=\"ml.eia1.medium\",\n",
    ")\n",
    "from sagemaker.workflow.steps import CreateModelStep\n",
    "\n",
    "create_best_model_steps = []\n",
    "entry_point='SageMaker_Pipeline_Component_Codes/Training/Evaluation.py',\n",
    "for i in range(n_models):\n",
    "    tuning_step_best_model = Model(image_uri = tuning_steps[i].tuner.estimator.image_uri, \n",
    "                                   source_dir = f\"s3://{pipeline_input_bucket}/codes/evaluation.tar.gz\",\n",
    "                                   # source_dir = build_parameters[\"single_model_evluation_source_dir\"],\n",
    "                                   entry_point = build_parameters[\"single_model_evluation_entry_point\"],\n",
    "                                   model_data = sagemaker.workflow.functions.Join(on='/', values=[hptune_training_output_path, tuning_steps[i].name[9:], tuning_steps[i].properties.BestTrainingJob.TrainingJobName, \"output/model.tar.gz\"]), \n",
    "                                   role = role,\n",
    "                                   sagemaker_session = sagemaker_session\n",
    "                                  )\n",
    "    \n",
    "    step_create_best_model = CreateModelStep(\n",
    "        name = f\"Getting-Best-{tuning_steps[i].name[9:]}-Model\",\n",
    "        model = tuning_step_best_model,\n",
    "        inputs = inputs\n",
    "    )\n",
    "    \n",
    "    create_best_model_steps.append(step_create_best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e07ac9",
   "metadata": {},
   "source": [
    "### Evaluating the best models from each hyperparameter tuning job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "909c00e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.transformer import Transformer\n",
    "from sagemaker.inputs import TransformInput\n",
    "from sagemaker.workflow.steps import TransformStep\n",
    "from sagemaker.workflow.properties import PropertyFile\n",
    "\n",
    "evaluation_steps = []\n",
    "\n",
    "for i in range(n_models):\n",
    "    transformer_dt = Transformer(\n",
    "        model_name = create_best_model_steps[i].properties.ModelName,\n",
    "        instance_type = build_parameters[\"evaluation_instance_type\"],\n",
    "        instance_count=1,\n",
    "        output_path=f\"{hptune_training_output_path}/{tuning_steps[i].name[9:]}/BestModel\",\n",
    "        base_transform_job_name = f\"{usecase}-evaluation-{tuning_steps[i].name[9:]}\",\n",
    "        env = {\"MODELS3LOCATION\":create_best_model_steps[i].properties.PrimaryContainer.ModelDataUrl, \n",
    "               \"MODELNAME\":build_parameters[\"model_specifications\"][f\"model{i}\"][\"model_name\"]}\n",
    "    )\n",
    "    evaluation_step = TransformStep(\n",
    "        name=f\"Evaluating-Best-{tuning_steps[i].name[9:]}-Model\",\n",
    "        transformer=transformer_dt,\n",
    "        inputs=TransformInput(data=step_process_evaluation.properties.ProcessingOutputConfig.Outputs[\"train\"].S3Output.S3Uri, \n",
    "                              # data_type = \"text/csv\"\n",
    "                             )\n",
    "    )\n",
    "    evaluation_steps.append(evaluation_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25b733d",
   "metadata": {},
   "source": [
    "### Getting the best model based on model performance metric on evaluation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ef5c276b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "for i in range(n_models):\n",
    "    inputs.append(ProcessingInput(sagemaker.workflow.functions.Join(on='/', values=[evaluation_steps[i].properties.TransformOutput.S3OutputPath, \"evaluation.csv.out\"]), destination=f\"/opt/ml/processing/input/model{i}\"))\n",
    "# inputs = inputs + [ProcessingInput(sagemaker.workflow.functions.Join(on='/', values=[\"s3:/\", pipeline_output_bucket, \"Training_Pipeline_Output\", \"Model_Performance_Metrics.csv\"]), destination=f\"/opt/ml/processing/metrics\")]\n",
    "inputs = inputs + [ProcessingInput(source = sagemaker.workflow.functions.Join(on='/', values=[\"s3:/\", pipeline_input_bucket, \"codes\", \"preprocessing_requirements.txt\"]), destination = \"/opt/ml/processing/input/requirements\")]\n",
    "# inputs = inputs + [ProcessingInput(source = sagemaker.workflow.functions.Join(on='/', values=[\"s3:/\", pipeline_input_bucket, \"codes\", \"preprocessing_requirements.txt\"]), destination = \"/opt/ml/processing/input/\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea07ff31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.properties import PropertyFile\n",
    "\n",
    "property_file = PropertyFile(\n",
    "    name=\"property_file\",\n",
    "    output_name=\"property_file\",\n",
    "    path=\"property_file.json\"\n",
    ")\n",
    "\n",
    "step_get_best_model = ProcessingStep(\n",
    "    name = \"Getting-Best-Model\",\n",
    "    description = \"Picking the best model based on the metric value calculated using evaluation data\",\n",
    "    processor = sklearn_processor,\n",
    "    inputs=inputs,\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"final_model\", source = \"/opt/ml/processing/final_model\", destination = evaluation_output_path),\n",
    "        ProcessingOutput(output_name=\"logs\", source = \"/opt/ml/processing/logs\", destination = evaluation_output_path),\n",
    "        ProcessingOutput(output_name=\"Metrics\", source = \"/opt/ml/processing/metrics_folder\", \n",
    "                         destination = functions.Join(on='/', values=[\"s3:/\", pipeline_output_bucket, \"Training_Pipeline_Output\"])\n",
    "                        ),\n",
    "                         # f\"s3://{pipeline_output_bucket}/Training_Pipeline_Output/\")\n",
    "        ProcessingOutput(output_name=\"Feature_Importance\", source = \"/opt/ml/processing/feature_importance\", \n",
    "                         destination = functions.Join(on='/', values=[\"s3:/\", pipeline_output_bucket, \"Training_Pipeline_Output\"])\n",
    "                        ),\n",
    "        ProcessingOutput(output_name=\"Confusion_Matrix\", source = \"/opt/ml/processing/confusion_matrix\", \n",
    "                         destination = functions.Join(on='/', values=[\"s3:/\", pipeline_output_bucket, \"Training_Pipeline_Output\"])\n",
    "                        ),\n",
    "        ProcessingOutput(output_name=\"Combined_Dashboard_Data\", source = \"/opt/ml/processing/Combined\", \n",
    "                         destination = functions.Join(on='/', values=[\"s3:/\", pipeline_output_bucket, \"Training_Pipeline_Output\"])\n",
    "                        ),\n",
    "        ProcessingOutput(output_name=\"property_file\", source = \"/opt/ml/processing/evaluation\", destination = evaluation_output_path)\n",
    "    ],\n",
    "    # code=\"SageMaker_Pipeline_Component_Codes/Training/Final_Model_Selection.py\",\n",
    "    code = f\"s3://{pipeline_input_bucket}/codes/{build_parameters['get_best_model_code_file_name']}\",\n",
    "    # depends_on = [step_dt_evaluation, step_lr_evaluation],\n",
    "    depends_on = evaluation_steps,\n",
    "    job_arguments = [\"--input_folder\", \"/opt/ml/processing/input\", \"--final_model_location\", \"/opt/ml/processing/final_model\", \n",
    "                     \"--logs_location\", \"/opt/ml/processing/logs\", \n",
    "                     # \"--model_metric_input_location\", \"/opt/ml/processing/metrics\", \n",
    "                     \"--model_metric_input_location\", sagemaker.workflow.functions.Join(on='/', values=[\"s3:/\", pipeline_output_bucket, \"Training_Pipeline_Output\", \"Model_Performance_Metrics.csv\"]),\n",
    "                     \"--model_metric_output_location\", \"/opt/ml/processing/metrics_folder\", \"--objective_metric\", objective_metric_name, \n",
    "                     \"--property_file_location\", \"/opt/ml/processing/evaluation\", \n",
    "                     \"--feature_importance_input_file_location\", sagemaker.workflow.functions.Join(on='/', values=[\"s3:/\", pipeline_output_bucket, \"Training_Pipeline_Output\", \"Feature_Importance.csv\"]),\n",
    "                     \"--feature_importance_output_file_location\", \"/opt/ml/processing/feature_importance\"\n",
    "                    ],\n",
    "    property_files=[property_file]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c51da6f",
   "metadata": {},
   "source": [
    "### Register best model in SageMaker model registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ff32cf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "\n",
    "register_best_model_steps = []\n",
    "\n",
    "for i in range(n_models):\n",
    "    model_details = build_parameters[\"model_specifications\"][f\"model{i}\"]\n",
    "    if model_details[\"model_type\"] == 'sklearn_model':\n",
    "        estimator = SKLearn(entry_point = \"\", \n",
    "                            \n",
    "                            instance_type = model_details[\"instance_type\"],\n",
    "                            framework_version = '0.20.0', \n",
    "                            image_uri = sklearn_image_uri,\n",
    "                            \n",
    "                            role = role\n",
    "                            )\n",
    "        register_best_model_step = RegisterModel(name=f\"RegisterBest{model_details['model_name']}Model\", \n",
    "                              estimator = estimator, \n",
    "                              # model_data=step_tuning.get_top_model_s3_uri(top_k=0, s3_bucket=model_path),\n",
    "                              model_data=sagemaker.workflow.functions.Join(on='/', values=[step_get_best_model.properties.ProcessingOutputConfig.Outputs[\"final_model\"].S3Output.S3Uri, \"model.tar.gz\"]),\n",
    "                              content_types=[\"text/csv\"],\n",
    "                              response_types=[\"text/csv\"],\n",
    "                              inference_instances=[model_details[\"instance_type\"]],\n",
    "                              transform_instances=[model_details[\"instance_type\"]],\n",
    "                              model_package_group_name = build_parameters[\"model_package_group_name\"],\n",
    "                              image_uri = sklearn_image_uri,\n",
    "                              # approval_status=\"Approved\",\n",
    "                              role=role,\n",
    "                              depends_on = []\n",
    "                             )\n",
    "        register_best_model_steps.append(register_best_model_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584810e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.conditions import ConditionEquals\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "from sagemaker.workflow.functions import JsonGet\n",
    "\n",
    "\n",
    "condition_steps = []\n",
    "for i in range(n_models):\n",
    "    model_details = build_parameters[\"model_specifications\"][f\"model{i}\"]\n",
    "    condition_equal = ConditionEquals(left = JsonGet(step_name=step_get_best_model.name, \n",
    "                                                   property_file=property_file, \n",
    "                                                   json_path=\"best_model_name\"),\n",
    "                                      right = model_details[\"model_name\"]\n",
    "                                     )\n",
    "    step_cond = ConditionStep(\n",
    "        name=f\"Is-{model_details['model_name']}-Best-Model\",\n",
    "        conditions=[condition_equal],\n",
    "        if_steps = [register_best_model_steps[i]],\n",
    "        )\n",
    "    condition_steps.append(step_cond)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bc6c23",
   "metadata": {},
   "source": [
    "### Model Given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b793f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if build_parameters[\"given_model_type\"] == \"sklearn\":\n",
    "#     estimator = SKLearn(entry_point = \"\", \n",
    "                        \n",
    "#                         instance_type = build_parameters[\"scoring_instance_type\"],\n",
    "#                         framework_version = '0.20.0', \n",
    "#                         image_uri = sklearn_image_uri,\n",
    "                        \n",
    "#                         role = role\n",
    "#                         )\n",
    "    \n",
    "#     register_given_model_step = RegisterModel(name=f\"RegisterGivenModel\", \n",
    "#                                              estimator = estimator, \n",
    "#                                              # model_data=step_tuning.get_top_model_s3_uri(top_k=0, s3_bucket=model_path),\n",
    "#                                              model_data=build_parameters[\"given_model_path\"],\n",
    "#                                              content_types=[\"text/csv\"],\n",
    "#                                              response_types=[\"text/csv\"],\n",
    "#                                              inference_instances=[build_parameters[\"scoring_instance_type\"]],\n",
    "#                                              transform_instances=[build_parameters[\"scoring_instance_type\"]],\n",
    "#                                              model_package_group_name = build_parameters[\"model_package_group_name\"],\n",
    "#                                              image_uri = sklearn_image_uri,\n",
    "#                                              approval_status=\"Approved\",\n",
    "#                                              role=role,\n",
    "#                                              depends_on = []\n",
    "#                                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279be51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_given_condition = ConditionEquals(left = build_parameters[\"model_given\"],\n",
    "#                                         right = \"No\"\n",
    "#                                        )\n",
    "# step_model_given_cond = ConditionStep(\n",
    "#     name=f\"Is-Model-Given\",\n",
    "#     conditions=[model_given_condition],\n",
    "#     # if_steps = [register_given_model_step],\n",
    "#     if_steps = [step_process] + tuning_steps + [step_process_evaluation] + create_best_model_steps + evaluation_steps + [step_get_best_model] + condition_steps,\n",
    "#     else_steps = [register_given_model_step]\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f80d77",
   "metadata": {},
   "source": [
    "### Arranging the steps inside pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cb5215bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "pipeline_name = f\"{usecase}-training\"\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        train_data,\n",
    "        test_data,\n",
    "        evaluation_data,\n",
    "        feature_selection_file,\n",
    "        pipeline_output_bucket,\n",
    "        #model_given,\n",
    "        #model_path,\n",
    "#         pipeline_output_path,\n",
    "        # processing_instance_count,\n",
    "        objective_metric_name,\n",
    "        # processing_code_location,\n",
    "        #training_instance_type,\n",
    "        #evaluation_instance_type,\n",
    "        processing_instance_type,\n",
    "        training_instance_type\n",
    "    ],\n",
    "#     steps=[step_process, step_process_evaluation, step_tuning_dt, step_tuning_lr, step_create_best_dt_model, step_cond],\n",
    "#     steps=[step_process_evaluation, step_process, step_tuning_dt, step_tuning_lr, step_create_best_dt_model, step_create_best_lr_model, step_dt_evaluation, step_lr_evaluation, step_cond]\n",
    "#     steps=[step_process, step_tuning_dt, step_process_evaluation, step_tuning_lr, step_create_best_dt_model, step_create_best_lr_model, step_dt_evaluation, step_lr_evaluation, step_get_best_model, step_register_best_model]\n",
    "#     steps = [step_cond]\n",
    "    steps = [step_process] + tuning_steps + [step_process_evaluation] + create_best_model_steps + evaluation_steps + [step_get_best_model] + condition_steps\n",
    "#     steps = [step_model_given_cond]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83c26c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "pipeline_name = f\"{build_parameters['usecase']}-training\"\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        train_data,\n",
    "        test_data,\n",
    "        evaluation_data,\n",
    "        feature_selection_file,\n",
    "        #model_given,\n",
    "        #model_path,\n",
    "#         pipeline_output_path,\n",
    "        # processing_instance_count,\n",
    "#         objective_metric_name,\n",
    "        # processing_code_location,\n",
    "        #training_instance_type,\n",
    "        #evaluation_instance_type,\n",
    "        processing_instance_type,\n",
    "#         training_instance_type\n",
    "    ] + training_instances + [objective_metric_name],\n",
    "#     steps=[step_process, step_process_evaluation, step_tuning_dt, step_tuning_lr, step_create_best_dt_model, step_cond],\n",
    "#     steps=[step_process_evaluation, step_process, step_tuning_dt, step_tuning_lr, step_create_best_dt_model, step_create_best_lr_model, step_dt_evaluation, step_lr_evaluation, step_cond]\n",
    "#     steps=[step_process, step_tuning_dt, step_process_evaluation, step_tuning_lr, step_create_best_dt_model, step_create_best_lr_model, step_dt_evaluation, step_lr_evaluation, step_get_best_model, step_register_best_model]\n",
    "#     steps = [step_cond]\n",
    "    steps = [step_process] + tuning_steps + best_model_steps,\n",
    "    sagemaker_session = pipeline_session\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4c0909",
   "metadata": {},
   "source": [
    "### Uploading the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c714ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:us-east-1:852619674999:pipeline/churn-training',\n",
       " 'ResponseMetadata': {'RequestId': 'be5f03eb-164a-43b1-ad0a-f7f62f3b84dc',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'be5f03eb-164a-43b1-ad0a-f7f62f3b84dc',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '82',\n",
       "   'date': 'Thu, 04 May 2023 11:33:48 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.upsert(role_arn=role)\n",
    "# execution = pipeline.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "182f707f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# execution = pipeline.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2db34f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4642de66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
