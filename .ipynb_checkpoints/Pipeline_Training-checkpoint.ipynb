{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf6f8b34",
   "metadata": {},
   "source": [
    "# Importing Libraries and Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a794c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "\n",
    "# Taking pipeline building configurations from config.json.\n",
    "# These are only for building and will not be available at \n",
    "# the runtime of the pipeline.\n",
    "with open(\"config.json\") as file:\n",
    "    build_parameters = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490c4b6d",
   "metadata": {},
   "source": [
    "### Setting Default Bucket for the Pipeline and getting region and role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30e6e580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Session.default_bucket of <sagemaker.workflow.pipeline_context.PipelineSession object at 0x7fd1a6f8c080>>\n"
     ]
    }
   ],
   "source": [
    "# Setting default bucket\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "pipeline_session = PipelineSession(default_bucket = build_parameters[\"output_bucket\"])\n",
    "sagemaker_session = sagemaker.Session()\n",
    "sagemaker_session.default_bucket = build_parameters[\"output_bucket\"]\n",
    "# sagemaker_session = sagemaker.Session()\n",
    "# bucket = \"sagemaker-output-bucket-us-east1\"   \n",
    "\n",
    "\n",
    "# print(sagemaker_session.default_bucket)\n",
    "print(pipeline_session.default_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f38d0aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::852619674999:role/service-role/AmazonSageMaker-ExecutionRole-20220427T124311\n"
     ]
    }
   ],
   "source": [
    "# Getting region and role\n",
    "region = boto3.Session().region_name\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "print(role)\n",
    "# print(sagemaker_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01085925",
   "metadata": {},
   "source": [
    "# Input Data Location Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f9bf7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Default location for the datasets\n",
    "train_data_uri = build_parameters[\"train_data\"]\n",
    "test_data_uri = build_parameters[\"test_data\"]\n",
    "evaluation_data_uri = build_parameters[\"evaluation_data\"]\n",
    "feature_selection_file_uri = build_parameters[\"feature_selection\"]\n",
    "\n",
    "\n",
    "# Parametrizing Data paths\n",
    "from sagemaker.workflow.parameters import ParameterInteger, ParameterString\n",
    "train_data = ParameterString(name=\"TrainData\", default_value = train_data_uri)\n",
    "test_data = ParameterString(name=\"TestData\", default_value = test_data_uri)\n",
    "evaluation_data = ParameterString(name=\"EvaluationData\", default_value = evaluation_data_uri)\n",
    "feature_selection_file = ParameterString(name = \"FeatureSelectionFile\", default_value = feature_selection_file_uri)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5dcac3f",
   "metadata": {},
   "source": [
    "### Handling output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52dbbf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Handling the output location\n",
    "# Default output location\n",
    "# pipeline_s3_output_bucket = f\"{usecase}-output-bucket-{region}\" \n",
    "# pipeline_s3_output_bucket = build_parameters[\"output_bucket\"]\n",
    "pipeline_output_bucket = build_parameters[\"output_bucket\"] \n",
    "\n",
    "# Making the output location runtime parameter\n",
    "# pipeline_output_bucket = ParameterString(name = \"PipelineOutputBucket\", default_value = pipeline_s3_output_bucket) \n",
    "sagemaker_session.default_bucket = pipeline_output_bucket\n",
    "\n",
    "# Creating the output bucket if it is not already present\n",
    "s3 = boto3.client('s3')\n",
    "buckets = [dictionary[\"Name\"] for dictionary in s3.list_buckets()['Buckets']]\n",
    "if pipeline_output_bucket not in buckets:\n",
    "    location = {'LocationConstraint': region}\n",
    "    response = s3.create_bucket(Bucket=bucket_name, CreateBucketConfiguration = location)\n",
    "\n",
    "\n",
    "from time import gmtime, strftime\n",
    "pipeline_start_time = strftime(\"%Y%m%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "from sagemaker.workflow import functions\n",
    "\n",
    "# These variables were written thinking that output path can be taken as parameter, yes it can be done,\n",
    "# but not all the pipeline steps accepts pipeline parameter as input, so we had to pick the output path from config\n",
    "# file instead of as parameter\n",
    "# processing_output_path = functions.Join(on='/', values=[\"s3:/\", pipeline_output_bucket, \"Training_Pipeline_Output\", pipeline_start_time, \"ProcessingOutput\"])\n",
    "# evaluation_processing_output_path = functions.Join(on='/', values=[\"s3:/\", pipeline_output_bucket, \"Training_Pipeline_Output\", pipeline_start_time, \"EvaluationProcessingOutput\"])\n",
    "# # hptune_training_output_path = functions.Join(on='/', values=[\"s3:/\", pipeline_output_bucket, \"Training_Pipeline_Output\", pipeline_start_time, \"HPTuneTrainingOutput\"])\n",
    "# hptune_training_output_path = f\"s3://{pipeline_s3_output_bucket}/Training_Pipeline_Output/{pipeline_start_time}/HPTuneTrainingOutput\"\n",
    "# evaluation_output_path = functions.Join(on='/', values=[\"s3:/\", pipeline_output_bucket, \"Training_Pipeline_Output\", pipeline_start_time, \"EvaluationOutput\"])\n",
    "\n",
    "\n",
    "processing_output_path = f\"s3://{pipeline_output_bucket}/Training_Pipeline_Output/{pipeline_start_time}/ProcessingOutput\"\n",
    "evaluation_processing_output_path = f\"s3://{pipeline_output_bucket}/Training_Pipeline_Output/{pipeline_start_time}/EvaluationProcessingOutput\"\n",
    "# hptune_training_output_path = functions.Join(on='/', values=[\"s3:/\", pipeline_output_bucket, \"Training_Pipeline_Output\", pipeline_start_time, \"HPTuneTrainingOutput\"])\n",
    "hptune_training_output_path = f\"s3://{pipeline_output_bucket}/Training_Pipeline_Output/{pipeline_start_time}/HPTuneTrainingOutput\"\n",
    "evaluation_output_path = f\"s3://{pipeline_output_bucket}/Training_Pipeline_Output/{pipeline_start_time}/EvaluationOutput\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b7bc60",
   "metadata": {},
   "source": [
    "# Preprocessing Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc6af82",
   "metadata": {},
   "source": [
    "#### 2.1 Loading preprocessing config.json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6956a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_preprocessing_path = os.path.join(\"Pipeline_Component_Codes\",\"Training\",\"1_Preprocessing\")\n",
    "with open(os.path.join(local_preprocessing_path, \"config.json\")) as file:\n",
    "    processing_build_parameters = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bd85ef",
   "metadata": {},
   "source": [
    "#### 2.2 Making parameter for processing machine type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e142a87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_instance_type = ParameterString(\n",
    "    name=\"ProcessingInstanceType\",\n",
    "    default_value=processing_build_parameters[\"machine_type\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7652235b",
   "metadata": {},
   "source": [
    "#### 2.3 Building the processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed3156ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if processing_build_parameters[\"processing_type\"] == \"sklearn\":\n",
    "    from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "    processor = SKLearnProcessor(\n",
    "        framework_version = processing_build_parameters[\"framework_version\"],\n",
    "        instance_type = processing_build_parameters[\"machine_type\"],\n",
    "        instance_count = processing_build_parameters[\"machine_count\"],\n",
    "        base_job_name = f\"{build_parameters['usecase']}-preprocessing\",\n",
    "        role=role\n",
    "    )\n",
    "elif processing_build_parameters[\"processing_type\"] == \"custom\":\n",
    "    from sagemaker.processor import Processor\n",
    "    processor = Processor(\n",
    "        image_uri = processing_build_parameters[\"image_uri\"],\n",
    "        instance_type = processing_build_parameters[\"machine_type\"],\n",
    "        instance_count = processing_build_parameters[\"machine_count\"],\n",
    "        base_job_name = f\"{build_parameters['usecase']}-preprocessing\",\n",
    "        role=role\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c45118e",
   "metadata": {},
   "source": [
    "#### 2.4 Building preprocessing step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef6abc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "    \n",
    "\n",
    "step_process = ProcessingStep(\n",
    "    name = \"preprocessing_full_data\",\n",
    "    description = \"Data preprocessing and splitting into train and test set\",\n",
    "    processor=processor,\n",
    "    inputs=[\n",
    "        ProcessingInput(source = train_data, destination=\"/opt/ml/processing/input/data\"),  \n",
    "        ProcessingInput(source=feature_selection_file, destination=\"/opt/ml/processing/input/feature_selection\")\n",
    "    ],\n",
    "    outputs=[\n",
    "        # Train\n",
    "        ProcessingOutput(output_name = \"train\", source=\"/opt/ml/processing/train\", \n",
    "                         destination = sagemaker_session.default_bucket\n",
    "#                          destination = sagemaker.workflow.functions.Join(on='/', values = [processing_output_path, \"data\"])\n",
    "                        ),\n",
    "        # Test\n",
    "        ProcessingOutput(output_name = \"test\", source=\"/opt/ml/processing/test\", \n",
    "                         destination = sagemaker_session.default_bucket\n",
    "#                          destination = sagemaker.workflow.functions.Join(on='/', values = [processing_output_path, \"data\"])\n",
    "                        ),\n",
    "        # Logs\n",
    "        ProcessingOutput(output_name = \"logs\", source=\"/opt/ml/processing/logss\", \n",
    "                         destination = sagemaker_session.default_bucket\n",
    "#                          destination = sagemaker.workflow.functions.Join(on='/', values = [processing_output_path, \"logs\"])\n",
    "                        ),\n",
    "    ],\n",
    "#     code=\"SageMaker_Pipeline_Component_Codes/Training/Training_Preprocessing.py\",\n",
    "    code=os.path.join(local_preprocessing_path, processing_build_parameters[\"entry_point\"]),\n",
    "    job_arguments = [\"--train_data_location\", \"/opt/ml/processing/input/data\", \n",
    "                     \"--feature_selection_file_location\", \"/opt/ml/processing/input/feature_selection/Feature_Selection.csv\", \n",
    "                     \"--target_column\", \"Churn\",\n",
    "                     \"--preprocessed_train_data_location\", \"/opt/ml/processing/train\", \n",
    "                     \"--preprocessed_test_data_location\", \"/opt/ml/processing/test\", \n",
    "                     \"--log_location\", \"/opt/ml/processing/logss\"\n",
    "                    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fdc56b",
   "metadata": {},
   "source": [
    "# 3. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a574a6",
   "metadata": {},
   "source": [
    "#### 3.1 Getting models from Local Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2dcfd150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Decision_Tree', 'Logistic_Regression', 'XGBoost']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "local_models_path = os.path.join(\"Pipeline_Component_Codes\", \"Training\", \"2_Models_HPTune\")\n",
    "models = []\n",
    "for directory in os.listdir(local_models_path):\n",
    "    if '.' not in directory: # Avoiding .ipynb_checkpoints\n",
    "        models.append(directory)\n",
    "\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcf8a85",
   "metadata": {},
   "source": [
    "#### 3.2 Reading config.json Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "20012418",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_details = {}\n",
    "for model in models:\n",
    "    with open(os.path.join(local_models_path, model, \"config.json\")) as file:\n",
    "        model_build_parameters = json.load(file)\n",
    "    model_details[model] = model_build_parameters\n",
    "\n",
    "# print(model_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1b72ef",
   "metadata": {},
   "source": [
    "#### 3.2 Input Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "61044413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine types\n",
    "training_instances = []\n",
    "for model in models:\n",
    "    training_instance_type = ParameterString(\n",
    "        name=f\"{model}_InstanceType\",\n",
    "        default_value=\"ml.m5.xlarge\"\n",
    "    )\n",
    "    training_instances.append(training_instance_type)\n",
    "\n",
    "# Objective metric\n",
    "objective_metric_name = ParameterString(name = \"ObjectiveMetric\", default_value = build_parameters[\"objective_metric\"])\n",
    "metric_definitions = [{\"Name\": objective_metric_name, \"Regex\": \"accuracy:([0-9\\\\.]+)\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecb40ee",
   "metadata": {},
   "source": [
    "#### 3.3 Creating the Estimators on which Tuning Will Happen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434c7be3",
   "metadata": {},
   "source": [
    "Sagemaker provides us docker conatiners for all the popular algorithms like they have scikit learn image for all the Scikit learn models, they have XGBoost image and they also have deep learning images as well. Here for the demo purpose we have used only three models, Logistic Regression, Decision Tree and XGBoost. So our need was not to build our own image. If any other Python library is needed we can mention those in **requirements.txt** file. We do not have to do anything more, pipeline will automatically install those in respective containers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5519386d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "instance_type is a PipelineVariable (<class 'sagemaker.workflow.parameters.ParameterString'>). Its interpreted value in execution time should not be of GPU types since GPU training is not supported for Scikit-Learn.\n",
      "The input argument instance_type of function (sagemaker.image_uris.retrieve) is a pipeline variable (<class 'sagemaker.workflow.parameters.ParameterString'>), which is not allowed. The default_value of this Parameter object will be used to override it. Please make sure the default_value is valid.\n",
      "instance_type is a PipelineVariable (<class 'sagemaker.workflow.parameters.ParameterString'>). Its interpreted value in execution time should not be of GPU types since GPU training is not supported for Scikit-Learn.\n",
      "The input argument instance_type of function (sagemaker.image_uris.retrieve) is a pipeline variable (<class 'sagemaker.workflow.parameters.ParameterString'>), which is not allowed. The default_value of this Parameter object will be used to override it. Please make sure the default_value is valid.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.sklearn import SKLearn\n",
    "from sagemaker.tuner import ContinuousParameter, IntegerParameter, CategoricalParameter, HyperparameterTuner, WarmStartConfig, WarmStartTypes\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.workflow.steps import TuningStep\n",
    "\n",
    "\n",
    "\n",
    "tuning_steps = []\n",
    "index = -1\n",
    "for model in models:\n",
    "    index = index + 1\n",
    "    current_model_details = model_details[model]\n",
    "    \n",
    "    # Creating the Estimators on which Tuning Will Happen\n",
    "    if current_model_details['model_type'] == 'sklearn_model':\n",
    "        estimator = SKLearn(source_dir = os.path.join(local_models_path, model),\n",
    "                            entry_point = current_model_details[\"entry_point\"], \n",
    "                            instance_type = training_instances[index], \n",
    "                            framework_version = '0.20.0', \n",
    "                            # output_path = f\"{hptune_training_output_path}/{model_details['model_name']}\",\n",
    "                            # output_path = pipeline_session.default_bucket,\n",
    "                            role = role\n",
    "                            )\n",
    "    \n",
    "    \n",
    "    # Getting the hyperparameters\n",
    "    hyperparameters = current_model_details[\"hyperparameters\"]\n",
    "    \n",
    "    hyperparameter_ranges = {}\n",
    "    for hyperparameter in hyperparameters:\n",
    "        if hyperparameters[hyperparameter][\"type\"] == \"continuous\":\n",
    "            hyperparameter_ranges[hyperparameter] = ContinuousParameter(min_value = hyperparameters[hyperparameter][\"min_value\"],\n",
    "                                                                        max_value = hyperparameters[hyperparameter][\"max_value\"],\n",
    "                                                                        scaling_type = hyperparameters[hyperparameter][\"scaling_type\"])\n",
    "        elif hyperparameters[hyperparameter][\"type\"] == \"categorical\":\n",
    "            hyperparameter_ranges[hyperparameter] = CategoricalParameter(hyperparameters[hyperparameter][\"values\"])\n",
    "        elif hyperparameters[hyperparameter][\"type\"] == \"integer\":\n",
    "            hyperparameter_ranges[hyperparameter] = IntegerParameter(min_value = hyperparameters[hyperparameter][\"min_value\"],\n",
    "                                                                     max_value = hyperparameters[hyperparameter][\"max_value\"])\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Making the hyperparameter tuner\n",
    "    tuner = HyperparameterTuner(\n",
    "        estimator = estimator,\n",
    "        objective_metric_name = objective_metric_name,\n",
    "        hyperparameter_ranges = hyperparameter_ranges,\n",
    "        metric_definitions = metric_definitions,\n",
    "        max_jobs=1,\n",
    "        max_parallel_jobs=1,\n",
    "        strategy = current_model_details[\"tuning_strategy\"],\n",
    "        base_tuning_job_name = current_model_details[\"model_name\"]\n",
    "        )\n",
    "    \n",
    "    \n",
    "    # Building the tuning step\n",
    "    step_tuning = TuningStep(\n",
    "        name = f\"hptuning_{current_model_details['model_name']}\",\n",
    "        tuner = tuner,\n",
    "        inputs={\n",
    "            \"train\": TrainingInput(\n",
    "                s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\"train\"].S3Output.S3Uri,\n",
    "                content_type=\"text/csv\"\n",
    "            ),\n",
    "            \"test\": TrainingInput(\n",
    "                s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\"test\"].S3Output.S3Uri,\n",
    "                content_type=\"text/csv\",\n",
    "            ),\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    tuning_steps.append(step_tuning)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4ba186",
   "metadata": {},
   "source": [
    "### Getting the best model from each hyperparameter tuning job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2ec53b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "from sagemaker.inputs import CreateModelInput\n",
    "inputs = CreateModelInput(\n",
    "    instance_type=build_parameters[\"evaluation_instance_type\"],\n",
    "    # accelerator_type=\"ml.eia1.medium\",\n",
    ")\n",
    "from sagemaker.workflow.steps import CreateModelStep\n",
    "\n",
    "create_best_model_steps = []\n",
    "entry_point='SageMaker_Pipeline_Component_Codes/Training/Evaluation.py',\n",
    "for i in range(n_models):\n",
    "    tuning_step_best_model = Model(image_uri = tuning_steps[i].tuner.estimator.image_uri, \n",
    "                                   source_dir = f\"s3://{pipeline_input_bucket}/codes/evaluation.tar.gz\",\n",
    "                                   # source_dir = build_parameters[\"single_model_evluation_source_dir\"],\n",
    "                                   entry_point = build_parameters[\"single_model_evluation_entry_point\"],\n",
    "                                   model_data = sagemaker.workflow.functions.Join(on='/', values=[hptune_training_output_path, tuning_steps[i].name[9:], tuning_steps[i].properties.BestTrainingJob.TrainingJobName, \"output/model.tar.gz\"]), \n",
    "                                   role = role,\n",
    "                                   sagemaker_session = sagemaker_session\n",
    "                                  )\n",
    "    \n",
    "    step_create_best_model = CreateModelStep(\n",
    "        name = f\"Getting-Best-{tuning_steps[i].name[9:]}-Model\",\n",
    "        model = tuning_step_best_model,\n",
    "        inputs = inputs\n",
    "    )\n",
    "    \n",
    "    create_best_model_steps.append(step_create_best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886a6cf7",
   "metadata": {},
   "source": [
    "### Evaluating the best models from each hyperparameter tuning job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6a55db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.transformer import Transformer\n",
    "from sagemaker.inputs import TransformInput\n",
    "from sagemaker.workflow.steps import TransformStep\n",
    "from sagemaker.workflow.properties import PropertyFile\n",
    "\n",
    "evaluation_steps = []\n",
    "\n",
    "for i in range(n_models):\n",
    "    transformer_dt = Transformer(\n",
    "        model_name = create_best_model_steps[i].properties.ModelName,\n",
    "        instance_type = build_parameters[\"evaluation_instance_type\"],\n",
    "        instance_count=1,\n",
    "        output_path=f\"{hptune_training_output_path}/{tuning_steps[i].name[9:]}/BestModel\",\n",
    "        base_transform_job_name = f\"{usecase}-evaluation-{tuning_steps[i].name[9:]}\",\n",
    "        env = {\"MODELS3LOCATION\":create_best_model_steps[i].properties.PrimaryContainer.ModelDataUrl, \n",
    "               \"MODELNAME\":build_parameters[\"model_specifications\"][f\"model{i}\"][\"model_name\"]}\n",
    "    )\n",
    "    evaluation_step = TransformStep(\n",
    "        name=f\"Evaluating-Best-{tuning_steps[i].name[9:]}-Model\",\n",
    "        transformer=transformer_dt,\n",
    "        inputs=TransformInput(data=step_process_evaluation.properties.ProcessingOutputConfig.Outputs[\"train\"].S3Output.S3Uri, \n",
    "                              # data_type = \"text/csv\"\n",
    "                             )\n",
    "    )\n",
    "    evaluation_steps.append(evaluation_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afec1f14",
   "metadata": {},
   "source": [
    "### Getting the best model based on model performance metric on evaluation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be1645c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "for i in range(n_models):\n",
    "    inputs.append(ProcessingInput(sagemaker.workflow.functions.Join(on='/', values=[evaluation_steps[i].properties.TransformOutput.S3OutputPath, \"evaluation.csv.out\"]), destination=f\"/opt/ml/processing/input/model{i}\"))\n",
    "# inputs = inputs + [ProcessingInput(sagemaker.workflow.functions.Join(on='/', values=[\"s3:/\", pipeline_output_bucket, \"Training_Pipeline_Output\", \"Model_Performance_Metrics.csv\"]), destination=f\"/opt/ml/processing/metrics\")]\n",
    "inputs = inputs + [ProcessingInput(source = sagemaker.workflow.functions.Join(on='/', values=[\"s3:/\", pipeline_input_bucket, \"codes\", \"preprocessing_requirements.txt\"]), destination = \"/opt/ml/processing/input/requirements\")]\n",
    "# inputs = inputs + [ProcessingInput(source = sagemaker.workflow.functions.Join(on='/', values=[\"s3:/\", pipeline_input_bucket, \"codes\", \"preprocessing_requirements.txt\"]), destination = \"/opt/ml/processing/input/\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d1e11ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.properties import PropertyFile\n",
    "\n",
    "property_file = PropertyFile(\n",
    "    name=\"property_file\",\n",
    "    output_name=\"property_file\",\n",
    "    path=\"property_file.json\"\n",
    ")\n",
    "\n",
    "step_get_best_model = ProcessingStep(\n",
    "    name = \"Getting-Best-Model\",\n",
    "    description = \"Picking the best model based on the metric value calculated using evaluation data\",\n",
    "    processor = sklearn_processor,\n",
    "    inputs=inputs,\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"final_model\", source = \"/opt/ml/processing/final_model\", destination = evaluation_output_path),\n",
    "        ProcessingOutput(output_name=\"logs\", source = \"/opt/ml/processing/logs\", destination = evaluation_output_path),\n",
    "        ProcessingOutput(output_name=\"Metrics\", source = \"/opt/ml/processing/metrics_folder\", \n",
    "                         destination = functions.Join(on='/', values=[\"s3:/\", pipeline_output_bucket, \"Training_Pipeline_Output\"])\n",
    "                        ),\n",
    "                         # f\"s3://{pipeline_output_bucket}/Training_Pipeline_Output/\")\n",
    "        ProcessingOutput(output_name=\"Feature_Importance\", source = \"/opt/ml/processing/feature_importance\", \n",
    "                         destination = functions.Join(on='/', values=[\"s3:/\", pipeline_output_bucket, \"Training_Pipeline_Output\"])\n",
    "                        ),\n",
    "        ProcessingOutput(output_name=\"Confusion_Matrix\", source = \"/opt/ml/processing/confusion_matrix\", \n",
    "                         destination = functions.Join(on='/', values=[\"s3:/\", pipeline_output_bucket, \"Training_Pipeline_Output\"])\n",
    "                        ),\n",
    "        ProcessingOutput(output_name=\"Combined_Dashboard_Data\", source = \"/opt/ml/processing/Combined\", \n",
    "                         destination = functions.Join(on='/', values=[\"s3:/\", pipeline_output_bucket, \"Training_Pipeline_Output\"])\n",
    "                        ),\n",
    "        ProcessingOutput(output_name=\"property_file\", source = \"/opt/ml/processing/evaluation\", destination = evaluation_output_path)\n",
    "    ],\n",
    "    # code=\"SageMaker_Pipeline_Component_Codes/Training/Final_Model_Selection.py\",\n",
    "    code = f\"s3://{pipeline_input_bucket}/codes/{build_parameters['get_best_model_code_file_name']}\",\n",
    "    # depends_on = [step_dt_evaluation, step_lr_evaluation],\n",
    "    depends_on = evaluation_steps,\n",
    "    job_arguments = [\"--input_folder\", \"/opt/ml/processing/input\", \"--final_model_location\", \"/opt/ml/processing/final_model\", \n",
    "                     \"--logs_location\", \"/opt/ml/processing/logs\", \n",
    "                     # \"--model_metric_input_location\", \"/opt/ml/processing/metrics\", \n",
    "                     \"--model_metric_input_location\", sagemaker.workflow.functions.Join(on='/', values=[\"s3:/\", pipeline_output_bucket, \"Training_Pipeline_Output\", \"Model_Performance_Metrics.csv\"]),\n",
    "                     \"--model_metric_output_location\", \"/opt/ml/processing/metrics_folder\", \"--objective_metric\", objective_metric_name, \n",
    "                     \"--property_file_location\", \"/opt/ml/processing/evaluation\", \n",
    "                     \"--feature_importance_input_file_location\", sagemaker.workflow.functions.Join(on='/', values=[\"s3:/\", pipeline_output_bucket, \"Training_Pipeline_Output\", \"Feature_Importance.csv\"]),\n",
    "                     \"--feature_importance_output_file_location\", \"/opt/ml/processing/feature_importance\"\n",
    "                    ],\n",
    "    property_files=[property_file]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf53637",
   "metadata": {},
   "source": [
    "### Register best model in SageMaker model registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd7545a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "\n",
    "register_best_model_steps = []\n",
    "\n",
    "for i in range(n_models):\n",
    "    model_details = build_parameters[\"model_specifications\"][f\"model{i}\"]\n",
    "    if model_details[\"model_type\"] == 'sklearn_model':\n",
    "        estimator = SKLearn(entry_point = \"\", \n",
    "                            \n",
    "                            instance_type = model_details[\"instance_type\"],\n",
    "                            framework_version = '0.20.0', \n",
    "                            image_uri = sklearn_image_uri,\n",
    "                            \n",
    "                            role = role\n",
    "                            )\n",
    "        register_best_model_step = RegisterModel(name=f\"RegisterBest{model_details['model_name']}Model\", \n",
    "                              estimator = estimator, \n",
    "                              # model_data=step_tuning.get_top_model_s3_uri(top_k=0, s3_bucket=model_path),\n",
    "                              model_data=sagemaker.workflow.functions.Join(on='/', values=[step_get_best_model.properties.ProcessingOutputConfig.Outputs[\"final_model\"].S3Output.S3Uri, \"model.tar.gz\"]),\n",
    "                              content_types=[\"text/csv\"],\n",
    "                              response_types=[\"text/csv\"],\n",
    "                              inference_instances=[model_details[\"instance_type\"]],\n",
    "                              transform_instances=[model_details[\"instance_type\"]],\n",
    "                              model_package_group_name = build_parameters[\"model_package_group_name\"],\n",
    "                              image_uri = sklearn_image_uri,\n",
    "                              # approval_status=\"Approved\",\n",
    "                              role=role,\n",
    "                              depends_on = []\n",
    "                             )\n",
    "        register_best_model_steps.append(register_best_model_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f974b615",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.conditions import ConditionEquals\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "from sagemaker.workflow.functions import JsonGet\n",
    "\n",
    "\n",
    "condition_steps = []\n",
    "for i in range(n_models):\n",
    "    model_details = build_parameters[\"model_specifications\"][f\"model{i}\"]\n",
    "    condition_equal = ConditionEquals(left = JsonGet(step_name=step_get_best_model.name, \n",
    "                                                   property_file=property_file, \n",
    "                                                   json_path=\"best_model_name\"),\n",
    "                                      right = model_details[\"model_name\"]\n",
    "                                     )\n",
    "    step_cond = ConditionStep(\n",
    "        name=f\"Is-{model_details['model_name']}-Best-Model\",\n",
    "        conditions=[condition_equal],\n",
    "        if_steps = [register_best_model_steps[i]],\n",
    "        )\n",
    "    condition_steps.append(step_cond)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef8bb4f",
   "metadata": {},
   "source": [
    "### Model Given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a4cd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if build_parameters[\"given_model_type\"] == \"sklearn\":\n",
    "#     estimator = SKLearn(entry_point = \"\", \n",
    "                        \n",
    "#                         instance_type = build_parameters[\"scoring_instance_type\"],\n",
    "#                         framework_version = '0.20.0', \n",
    "#                         image_uri = sklearn_image_uri,\n",
    "                        \n",
    "#                         role = role\n",
    "#                         )\n",
    "    \n",
    "#     register_given_model_step = RegisterModel(name=f\"RegisterGivenModel\", \n",
    "#                                              estimator = estimator, \n",
    "#                                              # model_data=step_tuning.get_top_model_s3_uri(top_k=0, s3_bucket=model_path),\n",
    "#                                              model_data=build_parameters[\"given_model_path\"],\n",
    "#                                              content_types=[\"text/csv\"],\n",
    "#                                              response_types=[\"text/csv\"],\n",
    "#                                              inference_instances=[build_parameters[\"scoring_instance_type\"]],\n",
    "#                                              transform_instances=[build_parameters[\"scoring_instance_type\"]],\n",
    "#                                              model_package_group_name = build_parameters[\"model_package_group_name\"],\n",
    "#                                              image_uri = sklearn_image_uri,\n",
    "#                                              approval_status=\"Approved\",\n",
    "#                                              role=role,\n",
    "#                                              depends_on = []\n",
    "#                                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f3a081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_given_condition = ConditionEquals(left = build_parameters[\"model_given\"],\n",
    "#                                         right = \"No\"\n",
    "#                                        )\n",
    "# step_model_given_cond = ConditionStep(\n",
    "#     name=f\"Is-Model-Given\",\n",
    "#     conditions=[model_given_condition],\n",
    "#     # if_steps = [register_given_model_step],\n",
    "#     if_steps = [step_process] + tuning_steps + [step_process_evaluation] + create_best_model_steps + evaluation_steps + [step_get_best_model] + condition_steps,\n",
    "#     else_steps = [register_given_model_step]\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c76a46",
   "metadata": {},
   "source": [
    "### Arranging the steps inside pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7e1501d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "pipeline_name = f\"{usecase}-training\"\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        train_data,\n",
    "        test_data,\n",
    "        evaluation_data,\n",
    "        feature_selection_file,\n",
    "        pipeline_output_bucket,\n",
    "        #model_given,\n",
    "        #model_path,\n",
    "#         pipeline_output_path,\n",
    "        # processing_instance_count,\n",
    "        objective_metric_name,\n",
    "        # processing_code_location,\n",
    "        #training_instance_type,\n",
    "        #evaluation_instance_type,\n",
    "        processing_instance_type,\n",
    "        training_instance_type\n",
    "    ],\n",
    "#     steps=[step_process, step_process_evaluation, step_tuning_dt, step_tuning_lr, step_create_best_dt_model, step_cond],\n",
    "#     steps=[step_process_evaluation, step_process, step_tuning_dt, step_tuning_lr, step_create_best_dt_model, step_create_best_lr_model, step_dt_evaluation, step_lr_evaluation, step_cond]\n",
    "#     steps=[step_process, step_tuning_dt, step_process_evaluation, step_tuning_lr, step_create_best_dt_model, step_create_best_lr_model, step_dt_evaluation, step_lr_evaluation, step_get_best_model, step_register_best_model]\n",
    "#     steps = [step_cond]\n",
    "    steps = [step_process] + tuning_steps + [step_process_evaluation] + create_best_model_steps + evaluation_steps + [step_get_best_model] + condition_steps\n",
    "#     steps = [step_model_given_cond]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4f7a917b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "pipeline_name = f\"{build_parameters['usecase']}-training\"\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        train_data,\n",
    "        test_data,\n",
    "        evaluation_data,\n",
    "        feature_selection_file,\n",
    "        #model_given,\n",
    "        #model_path,\n",
    "#         pipeline_output_path,\n",
    "        # processing_instance_count,\n",
    "#         objective_metric_name,\n",
    "        # processing_code_location,\n",
    "        #training_instance_type,\n",
    "        #evaluation_instance_type,\n",
    "        processing_instance_type,\n",
    "#         training_instance_type\n",
    "    ] + training_instances + [objective_metric_name],\n",
    "#     steps=[step_process, step_process_evaluation, step_tuning_dt, step_tuning_lr, step_create_best_dt_model, step_cond],\n",
    "#     steps=[step_process_evaluation, step_process, step_tuning_dt, step_tuning_lr, step_create_best_dt_model, step_create_best_lr_model, step_dt_evaluation, step_lr_evaluation, step_cond]\n",
    "#     steps=[step_process, step_tuning_dt, step_process_evaluation, step_tuning_lr, step_create_best_dt_model, step_create_best_lr_model, step_dt_evaluation, step_lr_evaluation, step_get_best_model, step_register_best_model]\n",
    "#     steps = [step_cond]\n",
    "    steps = [step_process] + tuning_steps,\n",
    "    sagemaker_session = pipeline_session\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c70483",
   "metadata": {},
   "source": [
    "### Uploading the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a76ea238",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:us-east-1:852619674999:pipeline/churn-training',\n",
       " 'ResponseMetadata': {'RequestId': '87825fc0-7dd9-4962-be0f-5c8a4c788878',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '87825fc0-7dd9-4962-be0f-5c8a4c788878',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '82',\n",
       "   'date': 'Wed, 03 May 2023 12:29:09 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.upsert(role_arn=role)\n",
    "# execution = pipeline.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "952303b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# execution = pipeline.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa766fa1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79852b27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
